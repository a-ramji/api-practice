---
title: "api-practice"
author: "Anna Ramji"
format: html
editor: visual
---

## Getting

list of my repos from github

```{r}
library(httr)
library(purrr)
library(dataRetrieval)
library(tidyverse)
library(janitor)
library(metajam)
library(here)
```

```{r}
r <- GET("https://api.github.com/users/a-ramji/repos")

my_repos_list <- content(r)

my_repos <- map_chr(my_repos_list, "full_name")

my_repos
```

## Exercise 1

-   Using `dataRetrieval` get the the discharge (ftÂ³/s) time-series for the Ventura River (site `11118500`) during the recent Hilary tropical storm from 2023-08-19 to 2023-08-22

-   Plot the time-series usgin `ggplot2`

-   What was the maximum discharge during this period of time?

```{r}
siteNumber <- "11118500"
parameterCd <- "00060" # Discharge
startDate <- "2023-08-19"
endDate <- "2023-08-22"

discharge <- readNWISdv(siteNumber, parameterCd, startDate, endDate) |> 
  clean_names() |> 
  mutate(discharge_ft3_s = x_00060_00003)


```

Plotting

```{r}
ggplot(data = discharge, aes(x = date, y = discharge_ft3_s)) +
  geom_point() +
  geom_line() +
  labs(x = "Date", y = "Discharge (ft^3/s", title = "Discharge of Ventura River during tropical storm Hilary") +
  theme_minimal()
```

The maximum discharge during this time period was

```{r}
max(discharge$discharge_ft3_s)
```

### **Bonus**

-   How would you try to determine when this stream gauge record started (read the documentation :) ) ?

    -   November 1908

    -   documentation says start date -- tab for summary of all available data

-   How does the max value compare to the discharge during the storm on 2023-01-09 ?

    -   

## **`metajam`**

The `metajam` R package relies on the dataONE API to download data **and** metadata into your R Environment. It is currently supporting KNB, ADC and EDI repositories because they rely on the metadata standard EML.

Short [intro](https://brunj7.github.io/eds214-slides/metajam_intro.html) to the package

### **Exercise 2**

Let's determine what percentage of Alaskan household are speaking only English!

The data: https://doi.org/10.5063/F1N58JPP

1.  Read the metadata on the website

2.  Start a new R script and write a code to:

    -   download the data `household_language.csv` using `metajam`

    -   Read the data into R using `metajam`

    -   compute the percentage of Alaskan household speaking only English from 2009 to 2015

3.  Create a plot to visualize this data

### Downloading the data:

```{r}
data_obj <- "https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A7fc6f6db-c5ea-426a-a743-1f2edafb43b8"
path <- "/Users/aramji/github/api-practice"
# you could also put "." to indicate your current location (here!)

download_d1_data(data_obj, path)
```

```{r}
# my_data <- read_d1_files("~/Desktop/doi_10.18739_A2DP3X__Alaska_Schools_Rentention2009_15")

```

### Reading in the data:

```{r}
ak_household_data <- read_d1_files("doi_10.5063_F1N58JPP__household_language__csv")
```

Extract data frame out of that list

```{r}
household_language_data <- ak_household_data$data
```

compute the percentage of Alaskan household speaking only English from 2009 to 2015

```{r}
with_percent_eng <- household_language_data |> 
  mutate(only_english_percent = (speak_only_english/total)) |> 
  filter(Year %in% c(2009:2015)) |> 
  group_by(Year) |> 
  summarize(only_eng_perc2 = mean(only_english_percent, na.rm = TRUE))
```

```{r}
percent_english <- household_language_data |> 
  select(Year, total, speak_only_english, city) |> 
  filter(Year %in% c(2009:2015)) |> 
  group_by(Year) |> 
  summarise(percent_only_english = (sum(speak_only_english, na.rm = TRUE) / sum(total, na.rm = TRUE)))

```

```{r}
# ggplot(with_percent_eng, aes(x = Year, y = only_english_percent)) +
#   geom_point()
```

```{r}
ggplot(percent_english, aes(x = Year, y = percent_only_english)) +
  geom_point() +
  geom_line() +
  labs(x = "Year", y = "% of houesholds where only English is spoken") +
  theme_minimal()
```

### **Bonus**

-   How does it compare to French?
